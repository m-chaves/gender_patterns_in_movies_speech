{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detected-trout",
   "metadata": {},
   "source": [
    "This notebook contains things that were not used but that might be useful in the end... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test splitting (original version)\n",
    "\n",
    "# Parameters \n",
    "# Dataset\n",
    "data = {'Movie':['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A','B','B','B','B','B','B','B','B','C','C','C','C','C'],'Movie2':['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A','B','B','B','B','B','B','B','B','C','C','C','C','C'],'Character': ['1', '1', '1', '1', '2', '2', '2', '3','4','4','4','1','1','2','2','3','1','1','2','3','4'],'Character2': ['1', '1', '1', '1', '2', '2', '2', '3','4','4','4','5','5','6','6','7','8','8','9','10','11'],'Dialogues': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h','i','j','k','l','m','n','o','p','q','r','s','t','u']}\n",
    "data = pd.DataFrame(data)\n",
    "# Variables where elements of each group should be represnted un train and test. \n",
    "# For example, we might want dialogues of each movie to be in both sets.\n",
    "each_in_both = ['Movie', 'Movie2']\n",
    "# Variables where elements of each group should be either in test or in train but not in both. \n",
    "# For example, we might want that if a movie is in the train set we don't want it in the test set.\n",
    "# Be carefull! More than one variable can be used, but if you selected, for instance, movie and actor, and let's assume movie1-actor1 dialogues were placed in the train set, movie1-actor2 could be in the test set. \n",
    "# This occurs because the groups are done at movie-actor level. \n",
    "not_in_both = ['Character']\n",
    "# Percent in the training \n",
    "train_percent = 0.8\n",
    "validation_percent = 0.1\n",
    "# Seed\n",
    "random_seed = None\n",
    "\n",
    "# Func\n",
    "\n",
    "# ----------------Case 1\n",
    "\n",
    "# Weights by group\n",
    "data_groups = data.groupby(each_in_both + not_in_both).size() / data.groupby(each_in_both).size()\n",
    "data_groups = data_groups.to_frame(name = 'weights').reset_index()\n",
    "\n",
    "print(data_groups)\n",
    "\n",
    "# Get the groups in train and test\n",
    "# Sample some percent of the groups\n",
    "train_groups = data_groups.groupby(each_in_both).sample(frac=train_percent, replace=False, weights=data_groups['weights'], random_state=random_seed).drop(columns = 'weights')\n",
    "# Groups that were npot selected in the train, go in the test\n",
    "test_groups = pd.merge(data_groups,train_groups,how='outer',indicator=True).query('_merge==\"left_only\"').drop(columns = ['_merge','weights'])\n",
    "\n",
    "# Get train and test\n",
    "train = pd.merge(data, train_groups)\n",
    "test = pd.merge(data, test_groups)\n",
    "\n",
    "print(train)\n",
    "print(test)\n",
    "\n",
    "len(train)/len(data)\n",
    "\n",
    "\n",
    "# -------------------------Case 2\n",
    "\n",
    "# Variables where elements of each group should be represnted un train and test. \n",
    "# For example, we might want dialogues of each movie to be in both sets.\n",
    "each_in_both = None\n",
    "# Variables where elements of each group should be either in test or in train but not in both\n",
    "# For example, we might want that if a movie is in the train set we don't want it in the test set.\n",
    "not_in_both = ['Movie']\n",
    "# Percent in the training \n",
    "train_percent = 0.8\n",
    "# Seed\n",
    "random_seed = None\n",
    "\n",
    "# Func\n",
    "\n",
    "# Weights by group\n",
    "data_groups = data.groupby(not_in_both).size() / len(data) #\n",
    "data_groups = data_groups.to_frame(name = 'weights').reset_index()\n",
    "\n",
    "# Get the groups in train and test\n",
    "# Sample some percent of the groups\n",
    "train_groups = data_groups.sample(frac=train_percent, replace=False, weights=data_groups['weights'], random_state=random_seed).drop(columns = 'weights') #\n",
    "# Groups that were npot selected in the train, go in the test\n",
    "test_groups = pd.merge(data_groups,train_groups,how='outer',indicator=True).query('_merge==\"left_only\"').drop(columns = ['_merge','weights'])\n",
    "\n",
    "# Get train and test\n",
    "train = pd.merge(data, train_groups)\n",
    "test = pd.merge(data, test_groups)\n",
    "\n",
    "print(train)\n",
    "print(test)\n",
    "\n",
    "len(train)/len(data)\n",
    "\n",
    "# -------------------------Case 3\n",
    "\n",
    "# Variables where elements of each group should be represnted un train and test. \n",
    "# For example, we might want dialogues of each movie to be in both sets.\n",
    "each_in_both = ['Movie', 'Movie2']\n",
    "# Variables where elements of each group should be either in test or in train but not in both\n",
    "# For example, we might want that if a movie is in the train set we don't want it in the test set.\n",
    "not_in_both = None\n",
    "# Percent in the training \n",
    "train_percent = 0.8\n",
    "# Seed\n",
    "random_seed = None\n",
    "\n",
    "# Get train and test\n",
    "train = data.groupby(each_in_both).sample(frac=train_percent, replace=False, random_state=random_seed)\n",
    "test = pd.merge(data,train,how='outer',indicator=True).query('_merge==\"left_only\"').drop(columns = ['_merge'])\n",
    "\n",
    "print(train_groups)\n",
    "print(test_groups)\n",
    "\n",
    "\n",
    "# -------------------------Case 4\n",
    "\n",
    "# Variables where elements of each group should be represnted un train and test. \n",
    "# For example, we might want dialogues of each movie to be in both sets.\n",
    "each_in_both = None\n",
    "# Variables where elements of each group should be either in test or in train but not in both. \n",
    "# For example, we might want that if a movie is in the train set we don't want it in the test set.\n",
    "# Be carefull! More than one variable can be used, but if you selected, for instance, movie and actor, and let's assume movie1-actor1 dialogues were placed in the train set, movie1-actor2 could be in the test set. \n",
    "# This occurs because the groups are done at movie-actor level. \n",
    "not_in_both = None\n",
    "# Percent in the training \n",
    "train_percent = 0.8\n",
    "# Seed\n",
    "random_seed = None\n",
    "\n",
    "# Get train and test\n",
    "train = data.sample(frac=train_percent, replace=False, random_state=random_seed)\n",
    "test = pd.merge(data,train,how='outer',indicator=True).query('_merge==\"left_only\"').drop(columns = ['_merge'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /nfshome/students/cm007951/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import myfunctions\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sys\n",
    "sys.path.append('../src/protoryNet/')\n",
    "from protoryNet import ProtoryNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "level-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfshome/students/cm007951/text-models\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "apart-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_trajectory(protorynet_model, list_of_sentences):\n",
    "    '''\n",
    "    Given a list of sentences (usually a list of prototypes), it returns the prediction for each of them. \n",
    "    Inputs:\n",
    "    protorynet_model: a protorynet model\n",
    "    list_of_sentences: a list of sentences\n",
    "    '''\n",
    "    pred = []\n",
    "    for prot in list_of_sentences:\n",
    "        pred.append(protorynet_model.predict([prot])[0])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train, val and testing splits\n",
    "\n",
    "# To see how this datasets were created see the ```protorynet_prototype_initializations.ipynb``` notebook. \n",
    "\n",
    "directory =  'datasets/cornell_corpus/cornell_prepro_characters_70train_20val_10test/'\n",
    "\n",
    "x_train = pickle.load(open(directory) + 'x_train', 'wb')\n",
    "x_val = pickle.load(open(directory) + 'x_val', 'wb')\n",
    "x_test = pickle.load(open(directory) + 'x_test', 'wb')\n",
    "y_train = pickle.load(open(directory) + 'y_train', 'wb')\n",
    "y_val = pickle.load(open(directory) + 'y_val', 'wb')\n",
    "y_test = pickle.load(open(directory) + 'y_test', 'wb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "answering-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# cornell_prepro_characters = pd.read_csv('datasets/cornell_corpus/cornell_prepro_characters.csv')\n",
    "\n",
    "# # Split data\n",
    "# X = cornell_prepro_characters['text_with_punctuation']\n",
    "# y = np.array(cornell_prepro_characters['gender'] == 'F').astype(int)\n",
    "\n",
    "# x_train, x_val, x_test, y_train, y_val, y_test = myfunctions.balanced_split_train_val_test(X, y, train_split = 0.7, val_split = 0.2, test_split = 0.1, random_seed = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fitting-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Saving to pickle format\n",
    "# directory =  'datasets/cornell_corpus/cornell_prepro_characters_70train_20val_10test/'\n",
    "\n",
    "# with open(directory +'x_train', 'wb') as f:\n",
    "#      pickle.dump(x_train, f)\n",
    "# with open(directory +'x_val', 'wb') as f:\n",
    "#      pickle.dump(x_val, f)\n",
    "# with open(directory +'x_test', 'wb') as f:\n",
    "#      pickle.dump(x_test, f)\n",
    "\n",
    "# with open(directory +'y_train', 'wb') as f:\n",
    "#      pickle.dump(y_train, f)\n",
    "# with open(directory +'y_val', 'wb') as f:\n",
    "#      pickle.dump(y_val, f)\n",
    "# with open(directory +'y_test', 'wb') as f:\n",
    "#      pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boring-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /nfshome/students/cm007951/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "new\n",
      "2022-07-26 12:29:12.127418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.135578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.136387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.137349: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-26 12:29:12.137621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.138401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.139134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.928070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.928970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.929744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 12:29:12.930478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21418 MB memory:  -> device: 0, name: GRID RTX6000-24C, pci bus id: 0000:02:04.0, compute capability: 7.5\n",
      "[db] model.input =  KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n",
      "[db] protoLayerName =  proto_layer\n",
      "[db] protoLayer =  <protoryNet.ProtoryNet.createModel.<locals>.prototypeLayer object at 0x7fdee03a2e10>\n",
      "[db] protoLayer.output =  (<KerasTensor: shape=(1, None, 10) dtype=float32 (created by layer 'proto_layer')>, <KerasTensor: shape=(10, 512) dtype=float32 (created by layer 'proto_layer')>)\n",
      "[db] distanceLayer.output =  KerasTensor(type_spec=TensorSpec(shape=(1, None, 10), dtype=tf.float32, name=None), name='distance_layer/PartitionedCall:0', description=\"created by layer 'distance_layer'\")\n",
      "Model: \"custom_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 512)               256797824 \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda)  (1, None, 512)           0         \n",
      "                                                                 \n",
      " proto_layer (prototypeLayer  ((1, None, 10),          5120      \n",
      " )                            (10, 512))                         \n",
      "                                                                 \n",
      " distance_layer (distanceLay  (1, None, 10)            0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(1, None, 128),          71168     \n",
      "                              (1, 128),                          \n",
      "                              (1, 128)]                          \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (1, 128)                 0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (1, 1)                    129       \n",
      "                                                                 \n",
      " tf.compat.v1.squeeze (TFOpL  (1,)                     0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " model (Functional)          ((1, None, 10),           256802944 \n",
      "                              (10, 512))                         \n",
      "                                                                 \n",
      " model_1 (Functional)        (1, None, 10)             256802944 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,874,241\n",
      "Trainable params: 256,874,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Initial prototypes execution time: 3.8591702421506247\n",
      "Epoch  0\n",
      "i =   0\n",
      "2022-07-26 12:33:31.662610: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8201\n",
      "5/5 [==============================] - 10s 79ms/step - loss: 1.8156\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "This is the best eval res, saving the model...\n",
      "saving model now = 2022-07-26 12:34:15.261937\n",
      "just saved\n",
      "i =   50\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.3321\n",
      "i =   100\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.4446\n",
      "i =   150\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 1.1028\n",
      "i =   200\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.4187\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   250\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.7596\n",
      "i =   300\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.8696\n",
      "i =   350\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8261\n",
      "i =   400\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0248\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   450\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 1.0552\n",
      "i =   500\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 1.0728\n",
      "i =   550\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.6182\n",
      "i =   600\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4881\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   650\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4841\n",
      "i =   700\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.4522\n",
      "i =   750\n",
      "14/14 [==============================] - 1s 78ms/step - loss: 0.5402\n",
      "i =   800\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.7907\n",
      "Evaluate on valid set:  0.4968814968814969\n",
      "i =   850\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.5257\n",
      "i =   900\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.7631\n",
      "i =   950\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 0.5179\n",
      "i =   1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6709\n",
      "Evaluate on valid set:  0.498960498960499\n",
      "i =   1050\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4794\n",
      "i =   1100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5451\n",
      "i =   1150\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.4855\n",
      "i =   1200\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.5675\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   1250\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.2613\n",
      "i =   1300\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.9910\n",
      "i =   1350\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 0.4949\n",
      "i =   1400\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.9311\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   1450\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.4193\n",
      "i =   1500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.4668\n",
      "i =   1550\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6608\n",
      "i =   1600\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5939\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   1650\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6746\n",
      "Epoch  1\n",
      "i =   0\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.8821\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   50\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.5869\n",
      "i =   100\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.5478\n",
      "i =   150\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.8720\n",
      "i =   200\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.4758\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   250\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.6331\n",
      "i =   300\n",
      "8/8 [==============================] - 1s 81ms/step - loss: 0.8187\n",
      "i =   350\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7328\n",
      "i =   400\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.8760\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   450\n",
      "8/8 [==============================] - 1s 80ms/step - loss: 0.9200\n",
      "i =   500\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.9574\n",
      "i =   550\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.6400\n",
      "i =   600\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4938\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   650\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5072\n",
      "i =   700\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.5194\n",
      "i =   750\n",
      "14/14 [==============================] - 1s 80ms/step - loss: 0.5714\n",
      "i =   800\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 0.7564\n",
      "Evaluate on valid set:  0.5051975051975052\n",
      "This is the best eval res, saving the model...\n",
      "saving model now = 2022-07-26 13:01:47.837244\n",
      "just saved\n",
      "i =   850\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.6094\n",
      "i =   900\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.7308\n",
      "i =   950\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.5587\n",
      "i =   1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.6918\n",
      "Evaluate on valid set:  0.5239085239085239\n",
      "This is the best eval res, saving the model...\n",
      "saving model now = 2022-07-26 13:04:22.397416\n",
      "just saved\n",
      "i =   1050\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5109\n",
      "i =   1100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5649\n",
      "i =   1150\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.4752\n",
      "i =   1200\n",
      "11/11 [==============================] - 1s 81ms/step - loss: 0.6246\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   1250\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.0788\n",
      "i =   1300\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.9331\n",
      "i =   1350\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 0.6012\n",
      "i =   1400\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.8478\n",
      "Evaluate on valid set:  0.501039501039501\n",
      "i =   1450\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.3645\n",
      "i =   1500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4458\n",
      "i =   1550\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5817\n",
      "i =   1600\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.7472\n",
      "Evaluate on valid set:  0.5426195426195426\n",
      "This is the best eval res, saving the model...\n",
      "saving model now = 2022-07-26 13:11:36.351797\n",
      "just saved\n",
      "i =   1650\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6768\n",
      "{'train_time': 39.47689822514852, 'history_validation_accuracy': [0.501039501039501, 0.5426195426195426], 'best_validation_accuracy': 0.5426195426195426, 'initial_prototypes': {0: 'I called your house like four times.', 1: \"I don't think people even noticed.\", 2: \"I'm the victim.\", 3: \"But I can't just drop everything and leave.\", 4: 'Oh, some of the scripts were so spirited!', 5: \"How'd you do on the science test?\", 6: \"I'm going to look around.\", 7: 'No de-fense.', 8: 'Baxter?', 9: 'Poor girl how could you do a thing like that?'}}\n"
     ]
    }
   ],
   "source": [
    "# !python code/train_protorynet.py --dataset_path=datasets/cornell_corpus/cornell_prepro_characters_70train_20val_10test/ --results_path=results/protorynet_models/ --results_prefix=cornell_prepro_characters_70train_20val_10test --epochs=2 --number_prototypes=10 --type_init=random --sample_size_sentences=20000 --init_prototypes_seed=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chicken-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing to use protorynet model's results\n",
    "# In this section there is no need to use the validation set, since we want to evaluate the predictions and accuracy in the test set.\n",
    "# The train set is necessary because we need to map the prototypes (which belong to the train set only)\n",
    "\n",
    "# Guarantee target variable is integer\n",
    "y_train = [int(y) for y in y_train]\n",
    "y_test = [int(y) for y in y_test]\n",
    "\n",
    "# Split text into lists of sentences \n",
    "x_train = myfunctions.split_sentences(x_train)\n",
    "x_test = myfunctions.split_sentences(x_test)\n",
    "\n",
    "# Make a list of sentences (only for training set)\n",
    "train_sentences = []\n",
    "for p in x_train:\n",
    "    train_sentences.extend(p)\n",
    "\n",
    "# We remove very short or very long sentences since they behave as outliers.\n",
    "train_sentences = [i for i in train_sentences if len(i)>5 and len(i)<100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reduced-sellers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[db] model.input =  KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n",
      "[db] protoLayerName =  proto_layer\n",
      "[db] protoLayer =  <protoryNet.ProtoryNet.createModel.<locals>.prototypeLayer object at 0x7f2cc025f8d0>\n",
      "[db] protoLayer.output =  (<KerasTensor: shape=(1, None, 10) dtype=float32 (created by layer 'proto_layer')>, <KerasTensor: shape=(10, 512) dtype=float32 (created by layer 'proto_layer')>)\n",
      "[db] distanceLayer.output =  KerasTensor(type_spec=TensorSpec(shape=(1, None, 10), dtype=tf.float32, name=None), name='distance_layer/PartitionedCall:0', description=\"created by layer 'distance_layer'\")\n",
      "Model: \"custom_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 512)               256797824 \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda)  (1, None, 512)           0         \n",
      "                                                                 \n",
      " proto_layer (prototypeLayer  ((1, None, 10),          5120      \n",
      " )                            (10, 512))                         \n",
      "                                                                 \n",
      " distance_layer (distanceLay  (1, None, 10)            0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(1, None, 128),          71168     \n",
      "                              (1, 128),                          \n",
      "                              (1, 128)]                          \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (1, 128)                 0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (1, 1)                    129       \n",
      "                                                                 \n",
      " tf.compat.v1.squeeze (TFOpL  (1,)                     0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      " model (Functional)          ((1, None, 10),           256802944 \n",
      "                              (10, 512))                         \n",
      "                                                                 \n",
      " model_1 (Functional)        (1, None, 10)             256802944 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,874,241\n",
      "Trainable params: 256,874,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "results_path = 'results/protorynet_models/'\n",
    "model_name = 'cornell_prepro_characters_70train_20val_10test__2epochs__10prototypes' \n",
    "model_path = results_path + model_name + '.h5'\n",
    "\n",
    "# Extract number of prototypes from models name\n",
    "number_prototypes = int(re.search('[0-9]*prototypes', model_name).group(0).replace('prototypes',''))\n",
    "\n",
    "# # Import Google Sentence encoder, to convert sentences into vector values\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "# model_sentence_encoder = hub.load(module_url)\n",
    "# def embed(input):\n",
    "#     return model_sentence_encoder(input)\n",
    "\n",
    "# #Compute embeddings of sentences\n",
    "# train_sentences_embedded = embed(train_sentences)\n",
    "\n",
    "# Load model\n",
    "pNet_saved = ProtoryNet()\n",
    "model = pNet_saved.createModel(np.zeros((number_prototypes, 512)), number_prototypes)\n",
    "model.load_weights(model_path)\n",
    "\n",
    "# Sentence embedding using the finetune embedder in the model\n",
    "train_sentences_embedded = pNet_saved.embed(train_sentences)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "preds_test, accuracy_test = pNet_saved.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "equipped-cisco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.9 s, sys: 125 ms, total: 27 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract final prototypes of the model\n",
    "final_prototypes = pNet_saved.showPrototypes(train_sentences, train_sentences_embedded, number_prototypes, printOutput=False, return_prototypes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "junior-lover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where you been?', 'Yeah.', 'Yeah.', \"Ok. Ok. Wow, you're goin' to Hawaii, that's great you're goin ?\", 'Where you goin?', 'What happened?', \"What's up?\", \"You talkin' to me about Ramada Inn?\", 'Which guy in Toledo are you talking about?', 'WHICH?', 'So what should I do with the pudding?', 'You should go on a trip.', \"You're goin' on a trip?\", \"What's with all this pudding, what is this?\", 'Good morning, Barry Why is it here?', 'Barry Barry?', \"That's not a piano I have a piano at home where'd you get it?\", 'What is this?', 'Yeah you got here early huh?', \"Why you wearin' a suit?\", 'You ok?', 'Hey, good morning, Barry.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.541902], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict example\n",
    "example = x_test[1]\n",
    "print(example)\n",
    "pNet_saved.predict(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handmade-collapse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_time': 39.47689822514852,\n",
       " 'history_validation_accuracy': [0.501039501039501, 0.5426195426195426],\n",
       " 'best_validation_accuracy': 0.5426195426195426,\n",
       " 'initial_prototypes': {0: 'I called your house like four times.',\n",
       "  1: \"I don't think people even noticed.\",\n",
       "  2: \"I'm the victim.\",\n",
       "  3: \"But I can't just drop everything and leave.\",\n",
       "  4: 'Oh, some of the scripts were so spirited!',\n",
       "  5: \"How'd you do on the science test?\",\n",
       "  6: \"I'm going to look around.\",\n",
       "  7: 'No de-fense.',\n",
       "  8: 'Baxter?',\n",
       "  9: 'Poor girl how could you do a thing like that?'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('results/protorynet_models/' + model_name + '.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-childhood",
   "metadata": {},
   "source": [
    "# Leave-some-movies-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unlikely-mistress",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /nfshome/students/cm007951/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import myfunctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "above-victim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfshome/students/cm007951/text-models\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "mounted-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_with_punctuation</th>\n",
       "      <th>gender</th>\n",
       "      <th>movieGroup</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They do not! I hope so. Let's go. Okay you're ...</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She died in her sleep three days ago. It was i...</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Six-fifty. Knock yourself out. That's all that...</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow Uh don't see it. There's no signature. But...</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of course, but there's more Uh, V.I.P. securit...</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>Maybe we should wait for Mr. Christy. The kill...</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>It's over twenty miles to the crossroads. Stev...</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>Gotta pee. You're lying on my bladder. Like wa...</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>I've got to go to town and pick up the trailer...</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>Everybody goes home! I'm going for 'em! I swea...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2404 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text_with_punctuation gender  movieGroup  \\\n",
       "0     They do not! I hope so. Let's go. Okay you're ...      F           1   \n",
       "1     She died in her sleep three days ago. It was i...      F           1   \n",
       "2     Six-fifty. Knock yourself out. That's all that...      M           5   \n",
       "3     Wow Uh don't see it. There's no signature. But...      F           1   \n",
       "4     Of course, but there's more Uh, V.I.P. securit...      M           1   \n",
       "...                                                 ...    ...         ...   \n",
       "2399  Maybe we should wait for Mr. Christy. The kill...      F           3   \n",
       "2400  It's over twenty miles to the crossroads. Stev...      M           3   \n",
       "2401  Gotta pee. You're lying on my bladder. Like wa...      F           3   \n",
       "2402  I've got to go to town and pick up the trailer...      M           3   \n",
       "2403  Everybody goes home! I'm going for 'em! I swea...      F           5   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  \n",
       "...      ...  \n",
       "2399       1  \n",
       "2400       0  \n",
       "2401       1  \n",
       "2402       0  \n",
       "2403       1  \n",
       "\n",
       "[2404 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "cornell_prepro_characters = pd.read_csv('datasets/cornell_corpus/cornell_prepro_characters.csv')\n",
    "\n",
    "# Take only the variables we need for training and leave-one-group-out\n",
    "cornell_prepro_characters_LOO = cornell_prepro_characters[['text_with_punctuation', 'gender', 'movieGroup']]\n",
    "\n",
    "# Target variable to 0 1\n",
    "cornell_prepro_characters_LOO['target'] = np.array(cornell_prepro_characters_LOO['gender'] == 'F').astype(int)\n",
    "\n",
    "# Save pandas dataset\n",
    "cornell_prepro_characters_LOO.to_csv('datasets/cornell_corpus/cornell_prepro_characters_LOO.csv', index=False)\n",
    "cornell_prepro_characters_LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sustainable-copper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536 1536\n",
      "384 384\n",
      "484 484\n",
      "1\n",
      "1510 1510\n",
      "378 378\n",
      "516 516\n",
      "2\n",
      "1531 1531\n",
      "383 383\n",
      "490 490\n",
      "3\n",
      "1561 1561\n",
      "391 391\n",
      "452 452\n",
      "4\n",
      "1553 1553\n",
      "389 389\n",
      "462 462\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import myfunctions\n",
    "\n",
    "# args\n",
    "dataset_path = 'datasets/cornell_corpus/cornell_prepro_characters_LOO.csv'\n",
    "results_path = 'results/protorynet_models/'\n",
    "results_prefix = 'cornell_prepro_characters'\n",
    "group_variable = 'movieGroup'\n",
    "text_variable = 'text_with_punctuation'\n",
    "target_variable = 'target'\n",
    "epochs = 1\n",
    "number_prototypes = 10 \n",
    "type_init = 'random' \n",
    "sample_size_sentences = 5000 \n",
    "init_prototypes_seed = 16\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Groups for leave-one-group-out\n",
    "groups = np.unique(data[group_variable])\n",
    "\n",
    "# Empty lists\n",
    "execution_time_list = []\n",
    "history_validation_accuracy_list = []\n",
    "maxEvalRes_list = []\n",
    "initial_prototypes_list = []\n",
    "accuracy_test_list = []\n",
    "final_prototypes_list = []\n",
    "text = []\n",
    "ground_truth = []\n",
    "\n",
    "# For each group\n",
    "for group in groups:\n",
    "    \n",
    "    results_name = '__'.join(['leave-one-group-out',\n",
    "                              results_prefix,\n",
    "                              group_variable + 'groupvariable',\n",
    "                              str(group) + 'groupnumber',\n",
    "                              str(epochs) + 'epochs', \n",
    "                              str(number_prototypes) + 'prototypes',\n",
    "                              type_init + 'type_init',\n",
    "                              str(sample_size_sentences) + 'sample_size_sentences',\n",
    "                              str(init_prototypes_seed) + 'init_prototypes_seed'])\n",
    "    \n",
    "    #-----------------------------\n",
    "    # Split train val test\n",
    "    #-----------------------------\n",
    "    \n",
    "    # Note that we need the 3 datasets because in the training process of protorynet, \n",
    "    # the model with the best metrics in validation is the one that is saved.\n",
    "    # In other words, we make a decision using data from validation. \n",
    "    # Therefore we need the test set to make the final assessment on accuracy.    \n",
    "    \n",
    "    train_data = data[data[group_variable] != group]\n",
    "    test_data = data[data[group_variable] == group]\n",
    "    \n",
    "    x = train_data[text_variable]\n",
    "    y = train_data[target_variable]\n",
    "    x_test = test_data[text_variable]\n",
    "    y_test = test_data[target_variable]\n",
    "    x_train, x_val, y_train, y_val  = myfunctions.balanced_split_train_val_test(x, y, train_split = 0.8, val_split = 0, test_split = 0.2, random_seed = 32)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Data preprocessing\n",
    "    # --------------------------\n",
    "\n",
    "    # Guarantee target variable is integer\n",
    "    y_train = [int(y) for y in y_train]\n",
    "    y_val = [int(y) for y in y_val]\n",
    "    y_test = [int(y) for y in y_test]\n",
    "\n",
    "    # Split text into lists of sentences \n",
    "    x_train = myfunctions.split_sentences(x_train)\n",
    "    x_val = myfunctions.split_sentences(x_val)\n",
    "    x_test = myfunctions.split_sentences(x_test)\n",
    "\n",
    "    # Make a list of sentences (only for training set)\n",
    "    train_sentences = []\n",
    "    for p in x_train:\n",
    "        train_sentences.extend(p)\n",
    "\n",
    "    # We remove very short or very long sentences since they behave as outliers.\n",
    "    train_sentences = [i for i in train_sentences if len(i)>5 and len(i)<100]\n",
    "\n",
    "    # Import Google Sentence encoder, to convert sentences into vector values\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "    model_sentence_encoder = hub.load(module_url)\n",
    "    def embed(input):\n",
    "        return model_sentence_encoder(input)\n",
    "\n",
    "    #Compute embeddings of sentences\n",
    "    train_sentences_embedded = embed(train_sentences)\n",
    "    \n",
    "    \n",
    "    # --------------------------\n",
    "    # Initialize prototypes\n",
    "    # --------------------------\n",
    "    initial_prototypes_embedded = myfunctions.initialize_prototypes(type_init, \n",
    "                                                        number_prototypes, \n",
    "                                                        train_sentences_embedded, \n",
    "                                                        sample_size = sample_size_sentences,\n",
    "                                                        init_prototypes_seed = init_prototypes_seed)\n",
    "\n",
    "    # --------------------------\n",
    "    # Train model\n",
    "    # --------------------------\n",
    "    \n",
    "    # Create model\n",
    "    pNet = ProtoryNet() \n",
    "    # Include the initial prototypes\n",
    "    model = pNet.createModel(initial_prototypes_embedded, k_protos = number_prototypes)\n",
    "\n",
    "    # Get the initial prototypes as sentences \n",
    "    start_time = time.time()\n",
    "    initial_prototypes = pNet.showPrototypes(train_sentences[0:sample_size_sentences], train_sentences_embedded[0:sample_size_sentences], number_prototypes, printOutput=False, return_prototypes = True)\n",
    "    print('Initial prototypes:', initial_prototypes)\n",
    "    execution_time = (time.time() - start_time) / 60\n",
    "    print('Initial prototypes execution time:', execution_time)\n",
    "\n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    maxEvalRes, history_validation_accuracy = pNet.train(x_train, y_train, x_val, y_val, \n",
    "                                                         epochs = epochs, \n",
    "                                                         saveModel = True, \n",
    "                                                         model_name = results_path + results_name, \n",
    "                                                         returnValidationAccuracy = True)\n",
    "    execution_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    # --------------------------\n",
    "    # Evaluate model\n",
    "    # --------------------------\n",
    "    model_path = results_path + results_name + '.h5'\n",
    "\n",
    "    # Load model\n",
    "    pNet_saved = ProtoryNet()\n",
    "    model = pNet_saved.createModel(np.zeros((number_prototypes, 512)), number_prototypes)\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    # Sentence embedding using the finetune embedder in the model\n",
    "    train_sentences_embedded = pNet_saved.embed(train_sentences)\n",
    "\n",
    "    # Evaluate the model on testing data\n",
    "    preds_test, accuracy_test = pNet_saved.evaluate(x_test, y_test)\n",
    "    \n",
    "    # Final_prototypes\n",
    "    final_prototypes = pNet_saved.showPrototypes(train_sentences, train_sentences_embedded, number_prototypes, printOutput=False, return_prototypes = True)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Save results\n",
    "    # --------------------------\n",
    "    \n",
    "    execution_time_list.append(execution_time)\n",
    "    history_validation_accuracy_list.append(history_validation_accuracy)\n",
    "    maxEvalRes_list.append(maxEvalRes)\n",
    "    initial_prototypes_list.append(initial_prototypes)\n",
    "    accuracy_test_list.append(accuracy_test)\n",
    "    final_prototypes_list.append(final_prototypes)\n",
    "    text.extend(x_test)\n",
    "    ground_truth.extend(y_test)\n",
    "    \n",
    "    results = {'train_time': execution_time_list,\n",
    "               'history_validation_accuracy': history_validation_accuracy_list,\n",
    "               'best_validation_accuracy': maxEvalRes_list,\n",
    "               'initial_prototypes': initial_prototypes_list,\n",
    "               'predictions_on_test': preds_test_list,\n",
    "               'text_test': text,\n",
    "               'ground_truth': true_values,\n",
    "               'test_accuracy': accuracy_test_list,\n",
    "               'final_prototypes': final_prototypes_list,\n",
    "               'args': args}\n",
    "    \n",
    "    print(results)\n",
    "    pickle.dump(results, open(results_path + results_name + \".pickle\", \"wb\" ))\n",
    "    \n",
    "    print('Finished for group:', group)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "solid-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(results_path + 'temporal'):\n",
    "    os.makedirs(results_path + 'temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elect-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(results_path + 'temporal_LOO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
